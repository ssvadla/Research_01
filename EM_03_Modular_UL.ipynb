{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EM_03_Modular_UL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdQ2Ejmlkz6Td4QDz1f7Xx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssvadla/Research_01/blob/main/EM_03_Modular_UL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-8Qf8LdzfGD"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJaX0B4lWLL7",
        "outputId": "f94f66d0-86e1-452c-8d0e-6c3cbc753832"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train1 = pd.read_csv('/content/drive/My Drive/Research/train_data1.csv')\n",
        "train2 = pd.read_csv('/content/drive/My Drive/Research/train_data2.csv')\n",
        "train3 = pd.read_csv('/content/drive/My Drive/Research/train_data3.csv')\n",
        "train4 = pd.read_csv('/content/drive/My Drive/Research/train_data4.csv')\n",
        "train5 = pd.read_csv('/content/drive/My Drive/Research/train_data5.csv')\n",
        "train6 = pd.read_csv('/content/drive/My Drive/Research/train_data6.csv')\n",
        "train7 = pd.read_csv('/content/drive/My Drive/Research/train_data7.csv')\n",
        "train8 = pd.read_csv('/content/drive/My Drive/Research/train_data8.csv')\n",
        "train9 = pd.read_csv('/content/drive/My Drive/Research/train_data9.csv')\n",
        "train10 = pd.read_csv('/content/drive/My Drive/Research/train_data10.csv')\n",
        "train_highKappa = pd.read_csv('/content/drive/My Drive/Research/train_data_highkappa.csv')\n",
        "train1.head()\n",
        "\n",
        "train = train1\n",
        "train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "for i in train_list:\n",
        "  #print(i)\n",
        "  train = train.append(i)\n",
        "\n",
        "\n",
        "train.sort_values(\"Sentence\", inplace = True)\n",
        "print(len(train))\n",
        "\n",
        "new_train = train.drop_duplicates(subset =\"Sentence\")\n",
        "\n",
        "train = new_train\n",
        "train['Target'].unique()\n",
        "train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "train['Target'].unique()\n",
        "\n",
        "#cleaning\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl= WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "  text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+',text)\n",
        "  text = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "  return text\n",
        "len(train['Sentence'])\n",
        "text = clean_text(train['Sentence'])\n",
        "type(text)\n",
        "len(text)\n",
        "\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "X_tfidf = tfidf_vect.fit_transform(train['Sentence'])\n",
        "print(X_tfidf.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "37711\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "(4416, 7374)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33rsHagDz1wm"
      },
      "source": [
        "test = pd.read_csv(r'/content/drive/My Drive/Research/test_data.csv')\n",
        "\n",
        "test['Target']=test['Target'].replace(['Others'],'Invalid')\n",
        "test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "test['Sentence'] = test['Sentence'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "t_p = tfidf_vect.transform(test['Sentence'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny7SbxbIz-cM",
        "outputId": "19ea83c9-4c57-4ae5-e6c2-d5eda16bc917"
      },
      "source": [
        "\n",
        "unlabel = pd.read_csv(r'/content/drive/My Drive/Research/Unlabeled_data.csv')\n",
        "#unlabel.head()\n",
        "\n",
        "del unlabel['Complete']\n",
        "del unlabel['Unnamed: 0']\n",
        "\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "unlabel['text'] = unlabel['text'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: TextBlob(x).words)\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "\n",
        "\n",
        "def index_reset(unlabel_2):\n",
        "  unlabel_2.reset_index(inplace=True)\n",
        "  del unlabel_2['index']\n",
        "  #print(unlabel_2.head())\n",
        "  return unlabel_2\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bakKN8gF1cdi"
      },
      "source": [
        "def feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words):\n",
        "  for term in train_and_unlabel_words_wo_duplicates:\n",
        "    #print(term)\n",
        "    freq_train = train_words.count(term) / len(train_words)\n",
        "    #print(freq_train)\n",
        "    freq_unlabel = unlabel_words.count(term) / len(unlabel_words)\n",
        "    #print(freq_unlabel)\n",
        "    if freq_unlabel == 0:\n",
        "      PF.append(term)\n",
        "    elif (freq_train // freq_unlabel) > Threshold_feature:\n",
        "      PF.append(term)\n",
        "    else:\n",
        "      NF.append(term)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSwX70qa1lgk"
      },
      "source": [
        "\n",
        "\n",
        "def RN_selection(RN, unlabel_1):\n",
        "  print(len(RN))\n",
        "  #Q = pd.DataFrame( )\n",
        "  iteration_RN = 0\n",
        "  RN_to_be_removed = []\n",
        "  pos_to_be_removed = []\n",
        "  freq_each_word_list = []\n",
        "  Q_pos = []\n",
        "\n",
        "  count = 0\n",
        "  if_count = 0\n",
        "\n",
        "  for doc in unlabel_1['text']:\n",
        "    #print(\"iteration_RN\",iteration_RN)\n",
        "    #print(doc)\n",
        "    doc_words = clean_text(doc)\n",
        "    doc_words_wo_duplicates = list(set(doc_words))\n",
        "    for_count = 0\n",
        "    \n",
        "    for each_doc_word in doc_words_wo_duplicates:\n",
        "      #print(each_doc_word)\n",
        "      for_count = for_count + 1\n",
        "      freq_each_doc_word = doc_words.count(each_doc_word)\n",
        "      freq_each_word_list.append(freq_each_doc_word)\n",
        "      #print(freq_each_doc_word)\n",
        "      #print(each_doc_word in PF)\n",
        "      if (freq_each_doc_word > 0) and (each_doc_word in PF):\n",
        "        #print(\"Yes writing ........\")\n",
        "        if_count = if_count + 1\n",
        "        pos_to_be_removed.append(count)\n",
        "        RN_to_be_removed.append(doc)\n",
        "        #print(\"breaking:::::::::::\")\n",
        "        break\n",
        "      \n",
        "    if for_count == len(doc_words_wo_duplicates):\n",
        "      Q_pos.append(count)\n",
        "\n",
        "    count = count + 1\n",
        "    iteration_RN = iteration_RN + 1\n",
        "  \n",
        "  RN.drop(pos_to_be_removed,axis=0,inplace=True)\n",
        "  #Q =  unlabel_1.loc[pos_to_be_removed,:]\n",
        "  #print(len(Q))\n",
        "  return RN,pos_to_be_removed, Q_pos, if_count\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDmtdb7a12e3"
      },
      "source": [
        "\n",
        "\n",
        "def classifier_select(train, Q, RN):\n",
        "  f1_score_list = []\n",
        "  per_CR_list = []\n",
        "\n",
        "  loop_variable =0\n",
        "  #for i in range(0,3):\n",
        "  while(1):  # comment it change the range \n",
        "    print(loop_variable)\n",
        "    p_and_RN = pd.concat([train,RN])\n",
        "    print(\"#######target unique\",np.unique(p_and_RN['Target']))\n",
        "    p_and_RN.reset_index(inplace=True,drop=True)\n",
        "    p_and_RN_vect = tfidf_vect.transform(p_and_RN['text'])\n",
        "    p_and_RN_vect_df=pd.DataFrame(p_and_RN_vect.toarray())\n",
        "\n",
        "    Q_vect = tfidf_vect.transform(Q['text'])\n",
        "    Q_vect_df=pd.DataFrame(Q_vect.toarray())\n",
        "\n",
        "    lgb_classifier = lgb.LGBMClassifier()\n",
        "    lgb_classifier.fit(p_and_RN_vect_df, p_and_RN['Target'])\n",
        "    np.unique(p_and_RN['Target'])\n",
        "\n",
        "    #checking the classifier if it gives best results\n",
        "    train_vect = tfidf_vect.transform(train['text'])\n",
        "    train_vect_df=pd.DataFrame(train_vect.toarray())\n",
        "    train_pred = lgb_classifier.predict(train_vect_df)\n",
        "    print(\"#############train pred\", np.unique(train_pred))\n",
        "    classified_negative = (train_pred.tolist()).count(-1)\n",
        "    print(\"unique predicted\", np.unique(train_pred))\n",
        "    print(\"classified_negative\",classified_negative)\n",
        "    print(\"len(train predic\",len(train_pred))\n",
        "    percentage = (classified_negative / len(train_pred)) * 100\n",
        "    model_list.append(lgb_classifier)\n",
        "    if percentage < percent_thresh:\n",
        "      print(\"???????\",percentage)\n",
        "      print(percent_thresh)\n",
        "      thresh_model_list.append(lgb_classifier)\n",
        "      test_pred = lgb_classifier.predict(t_p.toarray())\n",
        "      test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "      classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "      f1_score_test = f1_score(test['Target'],test_pred,average='weighted')\n",
        "      f1_score_list.append(f1_score_test)\n",
        "      per_CR_list.append(classification_report_test)\n",
        "\n",
        "\n",
        "          \n",
        "\n",
        "    Q_pred = lgb_classifier.predict(Q_vect_df)\n",
        "    np.unique(Q_pred)\n",
        "\n",
        "    \n",
        "\n",
        "    count_q = 0\n",
        "    total_q = 0 \n",
        "    out_pos_q = []\n",
        "    pos_q = []\n",
        "    for i in Q_pred:\n",
        "      #print(i)\n",
        "      if i == -1:\n",
        "        pos_q.append(count_q)\n",
        "        total_q = total_q + 1\n",
        "      else:\n",
        "        out_pos_q.append(count_q)\n",
        "\n",
        "      count_q = count_q + 1\n",
        "\n",
        "    Q.reset_index(inplace=True,drop=True)\n",
        "    W = Q.loc[pos_q,:]\n",
        "\n",
        "\n",
        "    if W.empty :\n",
        "      print(\"W is empty, came out of loop\")\n",
        "      break\n",
        "    else:\n",
        "      Q_new = Q.loc[out_pos_q,:]\n",
        "      Q = Q_new.copy(deep =True)\n",
        "      RN = pd.concat([RN,W])\n",
        "      RN.reset_index(inplace=True,drop=True)\n",
        "      loop_variable = loop_variable + 1\n",
        "    print(\"completed iteration\")\n",
        "  \n",
        "  result_dict['F1_score'].append(f1_score_list)\n",
        "  result_dict['CR'].append(per_CR_list)\n",
        "    \n",
        "\n",
        "  return lgb_classifier, model_list , thresh_model_list"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Wg303W17cP"
      },
      "source": [
        "model_list = []\n",
        "thresh_model_list = []\n",
        "UL_size_list = []\n",
        "\n",
        "result_dict = {\"F1_score\":[],\"CR\":[],\"Unlabel_size\":[]}\n",
        "Threshold_feature = 1\n",
        "percent_thresh = 5"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcF5a-Zf_4f7"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yQgkNRCQCeU"
      },
      "source": [
        "# def EM_training(unlabel_1,percent_thresh,train):\n",
        "#   print(train.columns)\n",
        "#   PF = []\n",
        "#   NF = []\n",
        "#   unlabel_1['Target']=-1\n",
        "\n",
        "#   train = train.rename(columns={'Sentence':'text'})\n",
        "  \n",
        "#   train['Target']= LabelEncoder().fit_transform(train['Target'])\n",
        "#   train_and_unlabel =  pd.concat([train,unlabel_1])\n",
        "#   print(len(train))\n",
        "#   print(len(unlabel_1))\n",
        "#   print(len(train_and_unlabel))\n",
        "#   train = train.rename(columns={'Sentence':'text'})\n",
        "\n",
        "#   train['Target']= LabelEncoder().fit_transform(train['Target'])\n",
        "#   train_and_unlabel =  pd.concat([train,unlabel_1])\n",
        "#   train_and_unlabel.reset_index(inplace=True) #commented now\n",
        "#   del train_and_unlabel['index']\n",
        "#   del train_and_unlabel['Unnamed: 0']\n",
        "#   train_and_unlabel_words = clean_text(train_and_unlabel['text'])\n",
        "#   train_and_unlabel_words_wo_duplicates = list(set(train_and_unlabel_words))\n",
        "#   train_words = clean_text(train['text'])\n",
        "#   train_words_wo_duplicates = list(set(train_words))\n",
        "#   unlabel_words = clean_text(unlabel_1['text'])\n",
        "#   unlabel_words_wo_duplicates = list(set(unlabel_words))\n",
        "\n",
        "#   feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words)\n",
        "\n",
        "#   RN = unlabel_1.copy(deep=True) \n",
        "\n",
        "#   RN,op_pos_to_be_removed, op_Q_pos, op_if_count = RN_selection(RN,unlabel_1) \n",
        "\n",
        "#   Q =  unlabel_1.loc[op_pos_to_be_removed,:]\n",
        "\n",
        "  \n",
        "#   lgb_classifier, op_model_list, op_thresh_model_list = classifier_select(train, Q, RN)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lHnJPpzRyIg"
      },
      "source": [
        "#unlabel_range = range(0,10000,5000)\n",
        "unlabel_range = [500,1000]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WgzChbk2sIi",
        "outputId": "5e1979d5-0082-4008-8f17-1e2040e90a68"
      },
      "source": [
        "#size = 500\n",
        "\n",
        "print(train.columns)\n",
        "for size in unlabel_range:\n",
        "  unlabel_1 = unlabel.loc[:size]\n",
        "  unlabel_1 = index_reset(unlabel_1)\n",
        "  unlabel_1_copy = unlabel_1.copy(deep = True)\n",
        "  x_un1 = tfidf_vect.transform(unlabel_1['text'])\n",
        "\n",
        "  #EM_training(unlabel_1,percent_thresh,train)\n",
        "\n",
        "  PF = []\n",
        "  NF = []\n",
        "  unlabel_1['Target']=-1\n",
        "\n",
        "  train = train.rename(columns={'Sentence':'text'})\n",
        "  \n",
        "  train['Target']= LabelEncoder().fit_transform(train['Target'])\n",
        "  train_and_unlabel =  pd.concat([train,unlabel_1])\n",
        "  print(len(train))\n",
        "  print(len(unlabel_1))\n",
        "  print(len(train_and_unlabel))\n",
        "  train = train.rename(columns={'Sentence':'text'})\n",
        "\n",
        "  train['Target']= LabelEncoder().fit_transform(train['Target'])\n",
        "  train_and_unlabel =  pd.concat([train,unlabel_1])\n",
        "  train_and_unlabel.reset_index(inplace=True) #commented now\n",
        "  del train_and_unlabel['index']\n",
        "  del train_and_unlabel['Unnamed: 0']\n",
        "  train_and_unlabel_words = clean_text(train_and_unlabel['text'])\n",
        "  train_and_unlabel_words_wo_duplicates = list(set(train_and_unlabel_words))\n",
        "  train_words = clean_text(train['text'])\n",
        "  train_words_wo_duplicates = list(set(train_words))\n",
        "  unlabel_words = clean_text(unlabel_1['text'])\n",
        "  unlabel_words_wo_duplicates = list(set(unlabel_words))\n",
        "\n",
        "  feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words)\n",
        "\n",
        "  RN = unlabel_1.copy(deep=True) \n",
        "\n",
        "  RN,op_pos_to_be_removed, op_Q_pos, op_if_count = RN_selection(RN,unlabel_1) \n",
        "\n",
        "  Q =  unlabel_1.loc[op_pos_to_be_removed,:]\n",
        "\n",
        "  result_dict['Unlabel_size'].append(size) \n",
        "  lgb_classifier, op_model_list, op_thresh_model_list = classifier_select(train, Q, RN)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'Sentence', 'Target'], dtype='object')\n",
            "4416\n",
            "501\n",
            "4917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "501\n",
            "0\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 12\n",
            "len(train predic 4416\n",
            "??????? 0.2717391304347826\n",
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completed iteration\n",
            "1\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 9\n",
            "len(train predic 4416\n",
            "??????? 0.20380434782608695\n",
            "5\n",
            "completed iteration\n",
            "2\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 10\n",
            "len(train predic 4416\n",
            "??????? 0.22644927536231885\n",
            "5\n",
            "completed iteration\n",
            "3\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 8\n",
            "len(train predic 4416\n",
            "??????? 0.18115942028985507\n",
            "5\n",
            "completed iteration\n",
            "4\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 8\n",
            "len(train predic 4416\n",
            "??????? 0.18115942028985507\n",
            "5\n",
            "completed iteration\n",
            "5\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 8\n",
            "len(train predic 4416\n",
            "??????? 0.18115942028985507\n",
            "5\n",
            "completed iteration\n",
            "6\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 11\n",
            "len(train predic 4416\n",
            "??????? 0.2490942028985507\n",
            "5\n",
            "completed iteration\n",
            "7\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 7\n",
            "len(train predic 4416\n",
            "??????? 0.1585144927536232\n",
            "5\n",
            "completed iteration\n",
            "8\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 8\n",
            "len(train predic 4416\n",
            "??????? 0.18115942028985507\n",
            "5\n",
            "completed iteration\n",
            "9\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 10\n",
            "len(train predic 4416\n",
            "??????? 0.22644927536231885\n",
            "5\n",
            "W is empty, came out of loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4416\n",
            "1001\n",
            "5417\n",
            "1001\n",
            "0\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 36\n",
            "len(train predic 4416\n",
            "??????? 0.8152173913043478\n",
            "5\n",
            "completed iteration\n",
            "1\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 47\n",
            "len(train predic 4416\n",
            "??????? 1.0643115942028987\n",
            "5\n",
            "completed iteration\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 52\n",
            "len(train predic 4416\n",
            "??????? 1.177536231884058\n",
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completed iteration\n",
            "3\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 53\n",
            "len(train predic 4416\n",
            "??????? 1.2001811594202898\n",
            "5\n",
            "completed iteration\n",
            "4\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 57\n",
            "len(train predic 4416\n",
            "??????? 1.2907608695652173\n",
            "5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "completed iteration\n",
            "5\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 57\n",
            "len(train predic 4416\n",
            "??????? 1.2907608695652173\n",
            "5\n",
            "completed iteration\n",
            "6\n",
            "#######target unique [-1  0  1  2  3  4  5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 54\n",
            "len(train predic 4416\n",
            "??????? 1.2228260869565217\n",
            "5\n",
            "W is empty, came out of loop\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o0CDWp3GtNC"
      },
      "source": [
        "with open(r'/content/drive/My Drive/EM/Result.txt', 'a') as writefile:\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" EM Results \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" final dict \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(str(result_dict))\n",
        "        writefile.write(\"\\n\")\n",
        "        "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8kRpMq82Cy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32548c38-789e-4b88-e5df-7f10796ec14d"
      },
      "source": [
        "\n",
        "\n",
        "for model in op_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  print(classification_report_test)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.3846    0.2597    0.3101        77\n",
            "           1     0.6957    0.6154    0.6531        26\n",
            "           2     0.6771    0.8951    0.7710       267\n",
            "           3     0.6122    0.3614    0.4545        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5926    0.4706    0.5246        34\n",
            "\n",
            "    accuracy                         0.6334       521\n",
            "   macro avg     0.5517    0.4096    0.4460       521\n",
            "weighted avg     0.6335    0.6334    0.6069       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4151    0.2857    0.3385        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6800    0.8914    0.7715       267\n",
            "           3     0.5490    0.3373    0.4179        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5556    0.4412    0.4918        34\n",
            "\n",
            "    accuracy                         0.6296       521\n",
            "   macro avg     0.5467    0.4051    0.4422       521\n",
            "weighted avg     0.6286    0.6296    0.6040       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4583    0.2857    0.3520        77\n",
            "           1     0.6800    0.6538    0.6667        26\n",
            "           2     0.6868    0.8951    0.7772       267\n",
            "           3     0.5400    0.3253    0.4060        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6353       521\n",
            "   macro avg     0.5494    0.4178    0.4518       521\n",
            "weighted avg     0.6363    0.6353    0.6110       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4151    0.2857    0.3385        77\n",
            "           1     0.7083    0.6538    0.6800        26\n",
            "           2     0.6793    0.8727    0.7639       267\n",
            "           3     0.5577    0.3494    0.4296        83\n",
            "           4     0.9167    0.3235    0.4783        34\n",
            "           5     0.5556    0.4412    0.4918        34\n",
            "\n",
            "    accuracy                         0.6276       521\n",
            "   macro avg     0.5475    0.4180    0.4546       521\n",
            "weighted avg     0.6297    0.6276    0.6072       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4615    0.3117    0.3721        77\n",
            "           1     0.7083    0.6538    0.6800        26\n",
            "           2     0.6888    0.8951    0.7785       267\n",
            "           3     0.5490    0.3373    0.4179        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.6000    0.4412    0.5085        34\n",
            "\n",
            "    accuracy                         0.6392       521\n",
            "   macro avg     0.5595    0.4190    0.4573       521\n",
            "weighted avg     0.6425    0.6392    0.6167       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4118    0.2727    0.3281        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6801    0.8839    0.7687       267\n",
            "           3     0.5490    0.3373    0.4179        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6257       521\n",
            "   macro avg     0.5485    0.4064    0.4438       521\n",
            "weighted avg     0.6292    0.6257    0.6027       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4348    0.2597    0.3252        77\n",
            "           1     0.6800    0.6538    0.6667        26\n",
            "           2     0.6742    0.8914    0.7677       267\n",
            "           3     0.5652    0.3133    0.4031        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.5556    0.4412    0.4918        34\n",
            "\n",
            "    accuracy                         0.6257       521\n",
            "   macro avg     0.5456    0.4076    0.4427       521\n",
            "weighted avg     0.6293    0.6257    0.6001       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4423    0.2987    0.3566        77\n",
            "           1     0.7083    0.6538    0.6800        26\n",
            "           2     0.6860    0.8839    0.7725       267\n",
            "           3     0.5283    0.3373    0.4118        83\n",
            "           4     0.8333    0.2941    0.4348        34\n",
            "           5     0.5926    0.4706    0.5246        34\n",
            "\n",
            "    accuracy                         0.6334       521\n",
            "   macro avg     0.5416    0.4198    0.4543       521\n",
            "weighted avg     0.6295    0.6334    0.6107       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4255    0.2597    0.3226        77\n",
            "           1     0.6800    0.6538    0.6667        26\n",
            "           2     0.6791    0.8876    0.7695       267\n",
            "           3     0.5094    0.3253    0.3971        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6257       521\n",
            "   macro avg     0.5379    0.4088    0.4401       521\n",
            "weighted avg     0.6220    0.6257    0.5989       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4255    0.2597    0.3226        77\n",
            "           1     0.6957    0.6154    0.6531        26\n",
            "           2     0.6799    0.8989    0.7742       267\n",
            "           3     0.5625    0.3253    0.4122        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.5926    0.4706    0.5246        34\n",
            "\n",
            "    accuracy                         0.6315       521\n",
            "   macro avg     0.5522    0.4091    0.4473       521\n",
            "weighted avg     0.6336    0.6315    0.6059       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4348    0.2597    0.3252        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6775    0.8577    0.7570       267\n",
            "           3     0.5870    0.3253    0.4186        83\n",
            "           4     0.8462    0.3235    0.4681        34\n",
            "           5     0.5926    0.4706    0.5246        34\n",
            "\n",
            "    accuracy                         0.6123       521\n",
            "   macro avg     0.5522    0.4075    0.4515       521\n",
            "weighted avg     0.6352    0.6123    0.6008       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4130    0.2468    0.3089        77\n",
            "           1     0.7083    0.6538    0.6800        26\n",
            "           2     0.6856    0.8577    0.7621       267\n",
            "           3     0.5532    0.3133    0.4000        83\n",
            "           4     0.8333    0.2941    0.4348        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6084       521\n",
            "   macro avg     0.5379    0.4052    0.4431       521\n",
            "weighted avg     0.6276    0.6084    0.5959       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4468    0.2727    0.3387        77\n",
            "           1     0.7619    0.6154    0.6809        26\n",
            "           2     0.6706    0.8539    0.7512       267\n",
            "           3     0.6047    0.3133    0.4127        83\n",
            "           4     0.9167    0.3235    0.4783        34\n",
            "           5     0.5200    0.3824    0.4407        34\n",
            "\n",
            "    accuracy                         0.6046       521\n",
            "   macro avg     0.5601    0.3945    0.4432       521\n",
            "weighted avg     0.6378    0.6046    0.5947       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.5098    0.3377    0.4062        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6942    0.8502    0.7643       267\n",
            "           3     0.6000    0.3253    0.4219        83\n",
            "           4     0.8462    0.3235    0.4681        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6200       521\n",
            "   macro avg     0.5641    0.4175    0.4633       521\n",
            "weighted avg     0.6555    0.6200    0.6164       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4565    0.2727    0.3415        77\n",
            "           1     0.7619    0.6154    0.6809        26\n",
            "           2     0.6778    0.8352    0.7483       267\n",
            "           3     0.5918    0.3494    0.4394        83\n",
            "           4     0.7857    0.3235    0.4583        34\n",
            "           5     0.6000    0.4412    0.5085        34\n",
            "\n",
            "    accuracy                         0.6046       521\n",
            "   macro avg     0.5534    0.4053    0.4538       521\n",
            "weighted avg     0.6376    0.6046    0.6010       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4468    0.2727    0.3387        77\n",
            "           1     0.7727    0.6538    0.7083        26\n",
            "           2     0.6820    0.8352    0.7508       267\n",
            "           3     0.5957    0.3373    0.4308        83\n",
            "           4     0.8333    0.2941    0.4348        34\n",
            "           5     0.5556    0.4412    0.4918        34\n",
            "\n",
            "    accuracy                         0.6027       521\n",
            "   macro avg     0.5552    0.4049    0.4507       521\n",
            "weighted avg     0.6396    0.6027    0.5993       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4375    0.2727    0.3360        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6697    0.8352    0.7433       267\n",
            "           3     0.5778    0.3133    0.4062        83\n",
            "           4     0.9167    0.3235    0.4783        34\n",
            "           5     0.6000    0.4412    0.5085        34\n",
            "\n",
            "    accuracy                         0.5988       521\n",
            "   macro avg     0.5613    0.4002    0.4484       521\n",
            "weighted avg     0.6352    0.5988    0.5930       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51Fi13UI2N5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e21bfd6-e3dc-4f72-9598-b784e51f4c09"
      },
      "source": [
        "for model in op_thresh_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  print(classification_report_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.3846    0.2597    0.3101        77\n",
            "           1     0.6957    0.6154    0.6531        26\n",
            "           2     0.6771    0.8951    0.7710       267\n",
            "           3     0.6122    0.3614    0.4545        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5926    0.4706    0.5246        34\n",
            "\n",
            "    accuracy                         0.6334       521\n",
            "   macro avg     0.5517    0.4096    0.4460       521\n",
            "weighted avg     0.6335    0.6334    0.6069       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4151    0.2857    0.3385        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6800    0.8914    0.7715       267\n",
            "           3     0.5490    0.3373    0.4179        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5556    0.4412    0.4918        34\n",
            "\n",
            "    accuracy                         0.6296       521\n",
            "   macro avg     0.5467    0.4051    0.4422       521\n",
            "weighted avg     0.6286    0.6296    0.6040       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4583    0.2857    0.3520        77\n",
            "           1     0.6800    0.6538    0.6667        26\n",
            "           2     0.6868    0.8951    0.7772       267\n",
            "           3     0.5400    0.3253    0.4060        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6353       521\n",
            "   macro avg     0.5494    0.4178    0.4518       521\n",
            "weighted avg     0.6363    0.6353    0.6110       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4151    0.2857    0.3385        77\n",
            "           1     0.7083    0.6538    0.6800        26\n",
            "           2     0.6793    0.8727    0.7639       267\n",
            "           3     0.5577    0.3494    0.4296        83\n",
            "           4     0.9167    0.3235    0.4783        34\n",
            "           5     0.5556    0.4412    0.4918        34\n",
            "\n",
            "    accuracy                         0.6276       521\n",
            "   macro avg     0.5475    0.4180    0.4546       521\n",
            "weighted avg     0.6297    0.6276    0.6072       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4615    0.3117    0.3721        77\n",
            "           1     0.7083    0.6538    0.6800        26\n",
            "           2     0.6888    0.8951    0.7785       267\n",
            "           3     0.5490    0.3373    0.4179        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.6000    0.4412    0.5085        34\n",
            "\n",
            "    accuracy                         0.6392       521\n",
            "   macro avg     0.5595    0.4190    0.4573       521\n",
            "weighted avg     0.6425    0.6392    0.6167       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4118    0.2727    0.3281        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6801    0.8839    0.7687       267\n",
            "           3     0.5490    0.3373    0.4179        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6257       521\n",
            "   macro avg     0.5485    0.4064    0.4438       521\n",
            "weighted avg     0.6292    0.6257    0.6027       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4348    0.2597    0.3252        77\n",
            "           1     0.6800    0.6538    0.6667        26\n",
            "           2     0.6742    0.8914    0.7677       267\n",
            "           3     0.5652    0.3133    0.4031        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.5556    0.4412    0.4918        34\n",
            "\n",
            "    accuracy                         0.6257       521\n",
            "   macro avg     0.5456    0.4076    0.4427       521\n",
            "weighted avg     0.6293    0.6257    0.6001       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4423    0.2987    0.3566        77\n",
            "           1     0.7083    0.6538    0.6800        26\n",
            "           2     0.6860    0.8839    0.7725       267\n",
            "           3     0.5283    0.3373    0.4118        83\n",
            "           4     0.8333    0.2941    0.4348        34\n",
            "           5     0.5926    0.4706    0.5246        34\n",
            "\n",
            "    accuracy                         0.6334       521\n",
            "   macro avg     0.5416    0.4198    0.4543       521\n",
            "weighted avg     0.6295    0.6334    0.6107       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4255    0.2597    0.3226        77\n",
            "           1     0.6800    0.6538    0.6667        26\n",
            "           2     0.6791    0.8876    0.7695       267\n",
            "           3     0.5094    0.3253    0.3971        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6257       521\n",
            "   macro avg     0.5379    0.4088    0.4401       521\n",
            "weighted avg     0.6220    0.6257    0.5989       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4255    0.2597    0.3226        77\n",
            "           1     0.6957    0.6154    0.6531        26\n",
            "           2     0.6799    0.8989    0.7742       267\n",
            "           3     0.5625    0.3253    0.4122        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.5926    0.4706    0.5246        34\n",
            "\n",
            "    accuracy                         0.6315       521\n",
            "   macro avg     0.5522    0.4091    0.4473       521\n",
            "weighted avg     0.6336    0.6315    0.6059       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4348    0.2597    0.3252        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6775    0.8577    0.7570       267\n",
            "           3     0.5870    0.3253    0.4186        83\n",
            "           4     0.8462    0.3235    0.4681        34\n",
            "           5     0.5926    0.4706    0.5246        34\n",
            "\n",
            "    accuracy                         0.6123       521\n",
            "   macro avg     0.5522    0.4075    0.4515       521\n",
            "weighted avg     0.6352    0.6123    0.6008       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4130    0.2468    0.3089        77\n",
            "           1     0.7083    0.6538    0.6800        26\n",
            "           2     0.6856    0.8577    0.7621       267\n",
            "           3     0.5532    0.3133    0.4000        83\n",
            "           4     0.8333    0.2941    0.4348        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6084       521\n",
            "   macro avg     0.5379    0.4052    0.4431       521\n",
            "weighted avg     0.6276    0.6084    0.5959       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4468    0.2727    0.3387        77\n",
            "           1     0.7619    0.6154    0.6809        26\n",
            "           2     0.6706    0.8539    0.7512       267\n",
            "           3     0.6047    0.3133    0.4127        83\n",
            "           4     0.9167    0.3235    0.4783        34\n",
            "           5     0.5200    0.3824    0.4407        34\n",
            "\n",
            "    accuracy                         0.6046       521\n",
            "   macro avg     0.5601    0.3945    0.4432       521\n",
            "weighted avg     0.6378    0.6046    0.5947       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.5098    0.3377    0.4062        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6942    0.8502    0.7643       267\n",
            "           3     0.6000    0.3253    0.4219        83\n",
            "           4     0.8462    0.3235    0.4681        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6200       521\n",
            "   macro avg     0.5641    0.4175    0.4633       521\n",
            "weighted avg     0.6555    0.6200    0.6164       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4565    0.2727    0.3415        77\n",
            "           1     0.7619    0.6154    0.6809        26\n",
            "           2     0.6778    0.8352    0.7483       267\n",
            "           3     0.5918    0.3494    0.4394        83\n",
            "           4     0.7857    0.3235    0.4583        34\n",
            "           5     0.6000    0.4412    0.5085        34\n",
            "\n",
            "    accuracy                         0.6046       521\n",
            "   macro avg     0.5534    0.4053    0.4538       521\n",
            "weighted avg     0.6376    0.6046    0.6010       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4468    0.2727    0.3387        77\n",
            "           1     0.7727    0.6538    0.7083        26\n",
            "           2     0.6820    0.8352    0.7508       267\n",
            "           3     0.5957    0.3373    0.4308        83\n",
            "           4     0.8333    0.2941    0.4348        34\n",
            "           5     0.5556    0.4412    0.4918        34\n",
            "\n",
            "    accuracy                         0.6027       521\n",
            "   macro avg     0.5552    0.4049    0.4507       521\n",
            "weighted avg     0.6396    0.6027    0.5993       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4375    0.2727    0.3360        77\n",
            "           1     0.7273    0.6154    0.6667        26\n",
            "           2     0.6697    0.8352    0.7433       267\n",
            "           3     0.5778    0.3133    0.4062        83\n",
            "           4     0.9167    0.3235    0.4783        34\n",
            "           5     0.6000    0.4412    0.5085        34\n",
            "\n",
            "    accuracy                         0.5988       521\n",
            "   macro avg     0.5613    0.4002    0.4484       521\n",
            "weighted avg     0.6352    0.5988    0.5930       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejks1jA-2Wik"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}
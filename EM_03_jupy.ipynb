{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EM_03_jupy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0rP6q/Twwm8F/cR0qFXxj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssvadla/Research_01/blob/main/EM_03_jupy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44u7_h2nWbM0"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl= WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "  text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+',text)\n",
        "  text = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "  return text\n",
        "    \n",
        "import pandas as pd\n",
        "unlabel = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Unlabeled_data.csv')\n",
        "unlabel.head()\n",
        "\n",
        "\n",
        "del unlabel['Complete']\n",
        "\n",
        "del unlabel['Unnamed: 0']\n",
        "\n",
        "unlabel.head()\n",
        "\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "unlabel['text'] = unlabel['text'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: TextBlob(x).words)\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "\n",
        "unlabel.head()\n",
        "\n",
        "\n",
        "len(unlabel)\n",
        "\n",
        "\n",
        "\n",
        "# unlabel_1 = unlabel.loc[:100]\n",
        "# unlabel_1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAjNjFoPWg-i"
      },
      "source": [
        "train1 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data1.csv')\n",
        "train2 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data2.csv')\n",
        "train3 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data3.csv')\n",
        "train4 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data4.csv')\n",
        "train5 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data5.csv')\n",
        "train6 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data6.csv')\n",
        "train7 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data7.csv')\n",
        "train8 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data8.csv')\n",
        "train9 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data9.csv')\n",
        "train10 = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data10.csv')\n",
        "train_highKappa = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\Train_data\\train_data_highkappa.csv')\n",
        "\n",
        "train1.head()\n",
        "train = train1\n",
        "train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "for i in train_list:\n",
        "    #print(i)\n",
        "    train = train.append(i)\n",
        "\n",
        "#unlabel_size = unlabel_size\n",
        "#Threshold=Threshold\n",
        "train.sort_values(\"Sentence\", inplace = True)\n",
        "print(len(train))\n",
        "\n",
        "train = train.drop_duplicates(subset =\"Sentence\")\n",
        "\n",
        "train['Target'].unique()\n",
        "\n",
        "\n",
        "train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "train['Target'].unique()\n",
        "print(len(train))\n",
        "train = train.rename(columns={'Sentence':'text'})\n",
        "all_Data_train_size = len(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Doaa7S0Wj_t"
      },
      "source": [
        "unlabel_size_list = []\n",
        "inner_thresh_model_f1score= []\n",
        "inner_thresh_model_CR = []\n",
        "inner_thresh_model_UL_size = []\n",
        "def EM_training(train,unlabel_1,percent_thresh):\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "    #from google.colab import drive\n",
        "\n",
        "#     drive.mount('/content/drive')\n",
        "\n",
        "#     train1 = pd.read_csv('/content/drive/My Drive/Research/train_data1.csv')\n",
        "#     train2 = pd.read_csv('/content/drive/My Drive/Research/train_data2.csv')\n",
        "#     train3 = pd.read_csv('/content/drive/My Drive/Research/train_data3.csv')\n",
        "#     train4 = pd.read_csv('/content/drive/My Drive/Research/train_data4.csv')\n",
        "#     train5 = pd.read_csv('/content/drive/My Drive/Research/train_data5.csv')\n",
        "#     train6 = pd.read_csv('/content/drive/My Drive/Research/train_data6.csv')\n",
        "#     train7 = pd.read_csv('/content/drive/My Drive/Research/train_data7.csv')\n",
        "#     train8 = pd.read_csv('/content/drive/My Drive/Research/train_data8.csv')\n",
        "#     train9 = pd.read_csv('/content/drive/My Drive/Research/train_data9.csv')\n",
        "#     train10 = pd.read_csv('/content/drive/My Drive/Research/train_data10.csv')\n",
        "#     train_highKappa = pd.read_csv('/content/drive/My Drive/Research/train_data_highkappa.csv')\n",
        "#     train1.head()\n",
        "\n",
        "#     train = train1\n",
        "#     train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "#     for i in train_list:\n",
        "#       #print(i)\n",
        "#       train = train.append(i)\n",
        "\n",
        "#     train.sort_values(\"Sentence\", inplace = True)\n",
        "#     print(len(train))\n",
        "#     new_train = train.drop_duplicates(subset =\"Sentence\")\n",
        "#     train = new_train\n",
        "#     train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "#     train['Target'].unique()\n",
        "\n",
        "    import nltk\n",
        "    import re\n",
        "    import string\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    stopword=nltk.corpus.stopwords.words('english')\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    wl= WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(text):\n",
        "      text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "      tokens = re.split('\\W+',text)\n",
        "      text = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "      return text\n",
        "\n",
        "    #len(train['Sentence'])\n",
        "    text = clean_text(train['text'])\n",
        "\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "    X_tfidf = tfidf_vect.fit_transform(train['text'])\n",
        "    print(X_tfidf.shape)\n",
        "\n",
        "    test = pd.read_csv(r'C:\\Users\\iia\\Documents\\Supriya\\test_data.csv')\n",
        "\n",
        "    test['Target']=test['Target'].replace(['Others'],'Invalid')\n",
        "    test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "    test['Sentence'] = test['Sentence'].str.replace('[^\\w\\s]','')\n",
        "    from nltk.corpus import stopwords\n",
        "    words = stopwords.words('english')\n",
        "    test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "    t_p = tfidf_vect.transform(test['Sentence'])\n",
        "\n",
        "\n",
        "\n",
        "#     unlabel = pd.read_csv(r'/content/drive/My Drive/Research/Unlabeled_data.csv')\n",
        "#     #unlabel.head()\n",
        "\n",
        "#     del unlabel['Complete']\n",
        "#     del unlabel['Unnamed: 0']\n",
        "\n",
        "#     unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "#     unlabel['text'] = unlabel['text'].str.replace('[^\\w\\s]','')\n",
        "#     from nltk.corpus import stopwords\n",
        "#     words = stopwords.words('english')\n",
        "#     unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "\n",
        "\n",
        "#     from textblob import TextBlob\n",
        "#     from textblob import Word\n",
        "#     nltk.download('wordnet')\n",
        "#     nltk.download('punkt')\n",
        "#     unlabel['text'] = unlabel['text'].apply(lambda x: TextBlob(x).words)\n",
        "#     unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "\n",
        "#     unlabel_1 = unlabel.loc[:500]\n",
        "\n",
        "\n",
        "    def index_reset(unlabel_2):\n",
        "      unlabel_2.reset_index(inplace=True)\n",
        "      del unlabel_2['index']\n",
        "      #print(unlabel_2.head())\n",
        "      return unlabel_2\n",
        "\n",
        "    unlabel_1 = index_reset(unlabel_1)\n",
        "    unlabel_1_copy = unlabel_1\n",
        "\n",
        "\n",
        "\n",
        "    x_un1 = tfidf_vect.transform(unlabel_1['text'])\n",
        "\n",
        "    unlabel_1['Target']=-1\n",
        "\n",
        "    train = train.rename(columns={'Sentence':'text'})\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    train['Target']= LabelEncoder().fit_transform(train['Target'])\n",
        "\n",
        "    train_and_unlabel =  pd.concat([train,unlabel_1])\n",
        "    print(len(train))\n",
        "    print(len(unlabel_1))\n",
        "    print(len(train_and_unlabel))\n",
        "\n",
        "    train_and_unlabel.reset_index(inplace=True)\n",
        "    del train_and_unlabel['index']\n",
        "    del train_and_unlabel['Unnamed: 0']\n",
        "\n",
        "\n",
        "    train_and_unlabel_words = clean_text(train_and_unlabel['text'])\n",
        "    train_and_unlabel_words_wo_duplicates = list(set(train_and_unlabel_words))\n",
        "    train_words = clean_text(train['text'])\n",
        "    train_words_wo_duplicates = list(set(train_words))\n",
        "    unlabel_words = clean_text(unlabel_1['text'])\n",
        "    unlabel_words_wo_duplicates = list(set(unlabel_words))\n",
        "\n",
        "\n",
        "    PF = []\n",
        "    NF = []\n",
        "    Threshold_feature = 1\n",
        "\n",
        "    def feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words):\n",
        "      for term in train_and_unlabel_words_wo_duplicates:\n",
        "        #print(term)\n",
        "        freq_train = train_words.count(term) / len(train_words)\n",
        "        #print(freq_train)\n",
        "        freq_unlabel = unlabel_words.count(term) / len(unlabel_words)\n",
        "        #print(freq_unlabel)\n",
        "        if freq_unlabel == 0:\n",
        "          PF.append(term)\n",
        "        elif (freq_train // freq_unlabel) > Threshold_feature:\n",
        "          PF.append(term)\n",
        "        else:\n",
        "          NF.append(term)\n",
        "        \n",
        "\n",
        "\n",
        "    feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def RN_selection(RN, unlabel_1):\n",
        "      unlabel_size = len(unlabel_1)\n",
        "      RN_len = len(RN)\n",
        "      iteration_RN = 0\n",
        "      RN_to_be_removed = []\n",
        "      pos_to_be_removed = []\n",
        "      freq_each_word_list = []\n",
        "      count = 0\n",
        "\n",
        "      for doc in unlabel_1['text']:\n",
        "        #print(\"iteration_RN\",iteration_RN)\n",
        "        #print(doc)\n",
        "        doc_words = clean_text(doc)\n",
        "        doc_words_wo_duplicates = list(set(doc_words))\n",
        "        for each_doc_word in doc_words_wo_duplicates:\n",
        "          #print(each_doc_word)\n",
        "          freq_each_doc_word = doc_words.count(each_doc_word)\n",
        "          freq_each_word_list.append(freq_each_doc_word)\n",
        "          #print(freq_each_doc_word)\n",
        "          #print(each_doc_word in PF)\n",
        "          if (freq_each_doc_word > 0) and (each_doc_word in PF):\n",
        "            #print(\"Yes writing ........\")\n",
        "            pos_to_be_removed.append(count)\n",
        "            RN_to_be_removed.append(doc)\n",
        "            print(\"breaking:::::::::::\")\n",
        "            break\n",
        "        count = count + 1\n",
        "        iteration_RN = iteration_RN + 1\n",
        "\n",
        "      RN.drop(pos_to_be_removed,axis=0,inplace=True)\n",
        "      #Q =  unlabel_1.loc[pos_to_be_removed,:]\n",
        "      #print(len(Q))\n",
        "      unlabel_size_list.append(unlabel_size)\n",
        "      with open(r'C:\\Users\\iia\\Documents\\Supriya\\EM_Results\\EM_01.txt', 'a') as writefile:\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" EM results \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\"unlabel_size = \")\n",
        "        writefile.write(str(unlabel_size))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\"RN_len\")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(str(RN_len))\n",
        "        \n",
        "      return RN,pos_to_be_removed\n",
        "\n",
        "    #unlabel_2 = unlabel_1[0:10] #comment it \n",
        "    RN = unlabel_1.copy(deep = True)\n",
        "    RN,pos_to_be_removed = RN_selection(RN,unlabel_1) # comment\n",
        "    #print(\"{{{{}}}}\",pos_to_be_removed)\n",
        "    Q =  unlabel_1.loc[pos_to_be_removed,:]\n",
        "\n",
        "    import lightgbm as lgb\n",
        "\n",
        "    def classifier_select(train, Q, RN):\n",
        "\n",
        "      loop_variable =0\n",
        "      #for i in range(0,3):\n",
        "      while(1):  # comment it change the range \n",
        "        print(loop_variable)\n",
        "        p_and_RN = pd.concat([train,RN])\n",
        "        print(\"#######target unique\",np.unique(p_and_RN['Target']))\n",
        "        p_and_RN.reset_index(inplace=True,drop=True)\n",
        "        p_and_RN_vect = tfidf_vect.transform(p_and_RN['text'])\n",
        "        p_and_RN_vect_df=pd.DataFrame(p_and_RN_vect.toarray())\n",
        "\n",
        "        Q_vect = tfidf_vect.transform(Q['text'])\n",
        "        Q_vect_df=pd.DataFrame(Q_vect.toarray())\n",
        "\n",
        "        lgb_classifier = lgb.LGBMClassifier()\n",
        "        lgb_classifier.fit(p_and_RN_vect_df, p_and_RN['Target'])\n",
        "        np.unique(p_and_RN['Target'])\n",
        "\n",
        "        #checking the classifier if it gives best results\n",
        "        train_vect = tfidf_vect.transform(train['text'])\n",
        "        train_vect_df=pd.DataFrame(train_vect.toarray())\n",
        "        train_pred = lgb_classifier.predict(train_vect_df)\n",
        "        print(\"#############train pred\", np.unique(train_pred))\n",
        "        classified_negative = (train_pred.tolist()).count(-1)\n",
        "        percentage = (classified_negative / len(train_pred)) * 100\n",
        "        model_list.append(lgb_classifier)\n",
        "        if percentage < percent_thresh:\n",
        "            test_pred = lgb_classifier.predict(t_p.toarray())\n",
        "            test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "            classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "            f1_score_test = f1_score(test['Target'],test_pred,average='weighted')\n",
        "            f1_score_list.append(f1_score_test)\n",
        "            print(classification_report_test)\n",
        "            thresh_model_list.append(lgb_classifier)\n",
        "            inner_thresh_model_f1score.append(f1_score_test)\n",
        "            inner_thresh_model_CR.append(classification_report_test)\n",
        "            inner_thresh_model_UL_size.append(unlabel_size)\n",
        "        \n",
        "\n",
        "\n",
        "        Q_pred = lgb_classifier.predict(Q_vect_df)\n",
        "        np.unique(Q_pred)\n",
        "\n",
        "\n",
        "\n",
        "        count_q = 0\n",
        "        total_q = 0 \n",
        "        out_pos_q = []\n",
        "        pos_q = []\n",
        "        for i in Q_pred:\n",
        "          #print(i)\n",
        "          if i == -1:\n",
        "            pos_q.append(count_q)\n",
        "            total_q = total_q + 1\n",
        "          else:\n",
        "            out_pos_q.append(count_q)\n",
        "\n",
        "          count_q = count_q + 1\n",
        "\n",
        "        Q.reset_index(inplace=True,drop=True)\n",
        "        W = Q.loc[pos_q,:]\n",
        "\n",
        "\n",
        "        if W.empty :\n",
        "          print(\"W is empty, came out of loop\")\n",
        "          break\n",
        "        else:\n",
        "          Q_new = Q.loc[out_pos_q,:]\n",
        "          Q = Q_new.copy(deep = True)\n",
        "          RN = pd.concat([RN,W])\n",
        "          RN.reset_index(inplace=True,drop=True)\n",
        "          loop_variable = loop_variable + 1\n",
        "        print(\"completed iteration\")\n",
        "\n",
        "\n",
        "      return lgb_classifier, model_list , thresh_model_list,f1_score_test\n",
        "\n",
        "    \n",
        "    \n",
        "    lgb_classifier, op_model_list, op_thresh_model_list,f1_score_test = classifier_select(train, Q, RN)\n",
        "    \n",
        "    \n",
        "    \n",
        "    with open(r'C:\\Users\\iia\\Documents\\Supriya\\EM_Results\\EM_01.txt', 'a') as writefile:\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" EM results \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\"unlabel_size list = \")\n",
        "        writefile.write(str(unlabel_size_list))\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\"f1_score_test = \")\n",
        "        writefile.write(str(f1_score_test))\n",
        " \n",
        "    \n",
        "    \n",
        "    return op_model_list, op_thresh_model_list,t_p, test\n",
        "\n",
        "   \n",
        "     \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHqL2e41WpEr"
      },
      "source": [
        "length_list = range(1000,100000,5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV0tMfVlWry4"
      },
      "source": [
        "percent_thresh = 5\n",
        "import numpy as np\n",
        "f1_score_list = []\n",
        "f1_score_thresh_list = []\n",
        "for size in length_list:\n",
        "    unlabel_1 = unlabel.loc[:size]\n",
        "    op_model_list, op_thresh_model_list,t_p, test = EM_training(train,unlabel_1,percent_thresh)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "for model in op_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  f1_score_test = f1_score(test['Target'],test_pred,average='weighted')\n",
        "  f1_score_list.append(f1_score_test)\n",
        "  print(classification_report_test)\n",
        "\n",
        "for model in op_thresh_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  f1_score_thresh = f1_score(test['Target'],test_pred,average='weighted')\n",
        "  f1_score_thresh_list.append(f1_score_thresh)\n",
        "  print(classification_report_test)\n",
        "\n",
        "    \n",
        "with open(r'C:\\Users\\iia\\Documents\\Supriya\\EM_Results\\EM_01.txt', 'a') as writefile:\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\" EM results \")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"f1_score_list = \")\n",
        "    writefile.write(str(f1_score_list))\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"f1_score_thresh_list = \")\n",
        "    writefile.write(str(f1_score_thresh_list))\n",
        "    \n",
        "with open(r'C:\\Users\\iia\\Documents\\Supriya\\EM_Results\\EM_01_variables.txt', 'a') as writefile:\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\" EM results \")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"inner_thresh_model_f1score = \")\n",
        "    writefile.write(str(inner_thresh_model_f1score))\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"inner_thresh_model_CR = \")\n",
        "    writefile.write(str(inner_thresh_model_CR))\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(\"inner_thresh_model_UL_size = \")\n",
        "    writefile.write(str(inner_thresh_model_UL_size))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EM_GMM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiKHnX1gyQ2uKVUNdIeFTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssvadla/Research_01/blob/main/EM_GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFzC2Ie4_Pzu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import os\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "wl= WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "  text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+',text)\n",
        "  text = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "  return text\n",
        "\n",
        "def load_data():\n",
        "\n",
        "    train1 = pd.read_csv('/content/drive/My Drive/Research/train_data1.csv')\n",
        "    train2 = pd.read_csv('/content/drive/My Drive/Research/train_data2.csv')\n",
        "    train3 = pd.read_csv('/content/drive/My Drive/Research/train_data3.csv')\n",
        "    train4 = pd.read_csv('/content/drive/My Drive/Research/train_data4.csv')\n",
        "    train5 = pd.read_csv('/content/drive/My Drive/Research/train_data5.csv')\n",
        "    train6 = pd.read_csv('/content/drive/My Drive/Research/train_data6.csv')\n",
        "    train7 = pd.read_csv('/content/drive/My Drive/Research/train_data7.csv')\n",
        "    train8 = pd.read_csv('/content/drive/My Drive/Research/train_data8.csv')\n",
        "    train9 = pd.read_csv('/content/drive/My Drive/Research/train_data9.csv')\n",
        "    train10 = pd.read_csv('/content/drive/My Drive/Research/train_data10.csv')\n",
        "    train_highKappa = pd.read_csv('/content/drive/My Drive/Research/train_data_highkappa.csv')\n",
        "\n",
        "    train = train1\n",
        "    train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "    for i in train_list:\n",
        "        print(i)\n",
        "        train = train.append(i)\n",
        "\n",
        "    train.sort_values(\"Sentence\", inplace = True)\n",
        "    new_train = train.drop_duplicates(subset =\"Sentence\")\n",
        "    train = new_train\n",
        "    train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "    print(\"all data shape @@@@@@@@@@\",train.shape)\n",
        "    print(type(train))\n",
        "    print(train['Target'].unique())\n",
        "    tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "    X_tfidf = tfidf_vect.fit_transform(train['Sentence'])\n",
        "    print(\"Shape of the vectyor\",X_tfidf.shape)\n",
        "    test = pd.read_csv(r'/content/drive/My Drive/Research/test_data.csv')\n",
        "    test['Target']=test['Target'].replace(['Others'],'Invalid')\n",
        "    test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "    test['Sentence'] = test['Sentence'].str.replace('[^\\w\\s]','') \n",
        "    words = stopwords.words('english')\n",
        "    test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "    t_p = tfidf_vect.transform(test['Sentence'])\n",
        "\t#X_train, x_val, Y_train, y_val = train_test_split(X_tfidf,train['Target'],test_size=0.26,random_state=42)\n",
        "    # print(\"Done\")\n",
        "    return X_tfidf, train['Target'],train\n",
        "  \n",
        "X, y , train = load_data()\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "print(type(X))\n",
        "print(type(y))\n",
        "#print(X.columns,y.columns)\n",
        "\n",
        "# X = train['Sentence']\n",
        "# y = train['Target']\n",
        "\n",
        "\n",
        "\n",
        "# iris = datasets.load_iris()\n",
        "\n",
        "# X = pd.DataFrame(iris.data)\n",
        "# X.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']\n",
        "\n",
        "y = pd.DataFrame(y)\n",
        "y.columns = ['target']\n",
        "\n",
        "print(\"###########\",y.columns)\n",
        "\n",
        "# Build the KMeans Model\n",
        "kmeans = KMeans(n_clusters=6)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "\n",
        "from scipy.stats import mode\n",
        "labels = np.zeros_like(clusters)\n",
        "\n",
        "for i in range(6):\n",
        "    cat = (clusters == i)\n",
        "    labels[cat] = mode(y.target[cat])[0]\n",
        "    \n",
        "acc = accuracy_score(y.target, labels)\n",
        "print('Accuracy = ', acc)\n",
        "\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# colormap = np.array(['red', 'lime', 'blue','yellow', 'green', 'black'])\n",
        "\n",
        "# # Plot the Original Classifications using Petal features\n",
        "# plt.subplot(2, 2, 1)\n",
        "# plt.scatter(X.text , c = colormap[y.Targets], s=40)\n",
        "# plt.title('Real Clusters')\n",
        "# plt.xlabel('Petal Length')\n",
        "# plt.ylabel('Petal Width')\n",
        "\n",
        "# # Plot KMeans Model Classifications\n",
        "# plt.subplot(2, 2, 2)\n",
        "# plt.scatter(X.Petal_Length, X.Petal_Width, c = colormap[labels], s = 40)\n",
        "# plt.title('KMeans Clusters')\n",
        "# plt.xlabel('Petal Length')\n",
        "# plt.ylabel('Petal Width')\n",
        "\n",
        "# General EM for GMM\n",
        "from sklearn import preprocessing\n",
        "\n",
        "X,y , train = load_data()\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "\n",
        "y = pd.DataFrame(y)\n",
        "y.columns = ['target']\n",
        "\n",
        "train['Target'] = LabelEncoder().fit_transform(train['Target'] )\n",
        "print(\"tye ----------\",type(X))\n",
        "hello = pd.DataFrame(X.toarray())\n",
        "print(type(y))\n",
        "\n",
        "# transform data such that the distribution mean = 0 and std = 1\n",
        "scaler = preprocessing.StandardScaler(with_mean=False)\n",
        "scaler.fit(hello)\n",
        "scaled_X = scaler.transform(hello)\n",
        "\n",
        "xs = pd.DataFrame(scaled_X) #, columns = X.columns)\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gmm = GaussianMixture(n_components=6)\n",
        "gmm_y = gmm.fit_predict(xs)\n",
        "\n",
        "labels = np.zeros_like(clusters)\n",
        "\n",
        "for i in range(6):\n",
        "    cat = (gmm_y == i)\n",
        "    labels[cat] = mode(y.target[cat])[0]\n",
        "    \n",
        "acc = accuracy_score(y.target, labels)\n",
        "print(\"Accuracy using GMM = \", acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# plt.subplot(2, 2, 3)\n",
        "# plt.scatter(X.text ,c = colormap[gmm_y], s = 40)\n",
        "# plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "# plt.title('GMM Clusters')\n",
        "# plt.xlabel('Petal Length')\n",
        "# plt.ylabel('Petal Width')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDSgNLJxFNdx"
      },
      "source": [
        "def load_data():\n",
        "\n",
        "    train1 = pd.read_csv('/content/drive/My Drive/Research/train_data1.csv')\n",
        "    train2 = pd.read_csv('/content/drive/My Drive/Research/train_data2.csv')\n",
        "    train3 = pd.read_csv('/content/drive/My Drive/Research/train_data3.csv')\n",
        "    train4 = pd.read_csv('/content/drive/My Drive/Research/train_data4.csv')\n",
        "    train5 = pd.read_csv('/content/drive/My Drive/Research/train_data5.csv')\n",
        "    train6 = pd.read_csv('/content/drive/My Drive/Research/train_data6.csv')\n",
        "    train7 = pd.read_csv('/content/drive/My Drive/Research/train_data7.csv')\n",
        "    train8 = pd.read_csv('/content/drive/My Drive/Research/train_data8.csv')\n",
        "    train9 = pd.read_csv('/content/drive/My Drive/Research/train_data9.csv')\n",
        "    train10 = pd.read_csv('/content/drive/My Drive/Research/train_data10.csv')\n",
        "    train_highKappa = pd.read_csv('/content/drive/My Drive/Research/train_data_highkappa.csv')\n",
        "\n",
        "    train = train1\n",
        "    train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "    for i in train_list:\n",
        "        print(i)\n",
        "        train = train.append(i)\n",
        "\n",
        "    train.sort_values(\"Sentence\", inplace = True)\n",
        "    new_train = train.drop_duplicates(subset =\"Sentence\")\n",
        "    train = new_train\n",
        "    train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "    print(\"all data shape @@@@@@@@@@\",train.shape)\n",
        "    print(type(train))\n",
        "    print(train['Target'].unique())\n",
        "    tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "    X_tfidf = tfidf_vect.fit_transform(train['Sentence'])\n",
        "    print(\"Shape of the vectyor\",X_tfidf.shape)\n",
        "    test = pd.read_csv(r'/content/drive/My Drive/Research/test_data.csv')\n",
        "    test['Target']=test['Target'].replace(['Others'],'Invalid')\n",
        "    test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "    test['Sentence'] = test['Sentence'].str.replace('[^\\w\\s]','') \n",
        "    words = stopwords.words('english')\n",
        "    test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "    t_p = tfidf_vect.transform(test['Sentence'])\n",
        "\t#X_train, x_val, Y_train, y_val = train_test_split(X_tfidf,train['Target'],test_size=0.26,random_state=42)\n",
        "    # print(\"Done\")\n",
        "    return X_tfidf, train['Target'],train,t_p,test"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmbCcHjkFb4F"
      },
      "source": [
        "X_tfidf, train['Target'],train,t_p,test = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgOBTY9iFNgt"
      },
      "source": [
        "test_target = test['Target']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCSrJojPFNkP",
        "outputId": "b047bc5e-7a4e-4577-813e-991ad44a82f9"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Sentence', 'Target'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTSPV4KCExVc"
      },
      "source": [
        "test_df = pd.DataFrame(t_p.toarray())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy1aWSF9FyjW"
      },
      "source": [
        "gmm_test_pred = gmm.predict(test_df)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg-z8zPJGdE7"
      },
      "source": [
        "test = pd.DataFrame(test)\n",
        "test.columns = ['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR2QRE1AH7ME",
        "outputId": "7088a120-df49-4a5d-c71e-009d2aad7886"
      },
      "source": [
        "len(gmm_test_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "521"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD29CKJJIAPx",
        "outputId": "6a822d47-2a96-46a2-e162-ebeb7215c8cc"
      },
      "source": [
        "len(cat)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "521"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQCw2qCsIDPo",
        "outputId": "5585f3b8-a853-4936-bc87-c088465129c1"
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "521"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3w8TnQkIIB2"
      },
      "source": [
        "test_labels = np.zeros_like(gmm_test_pred)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbUcO_tWIqFI"
      },
      "source": [
        "test_target = LabelEncoder().fit_transform(test_target)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPh5f9LMExYX",
        "outputId": "669ca03d-a3db-416c-a5f8-864f497bcee5"
      },
      "source": [
        "\n",
        "#test_labels = np.zeros_like(clusters)\n",
        "\n",
        "for i in range(6):\n",
        "    cat = (gmm_test_pred == i)\n",
        "    test_labels[cat] = mode(test_target[cat])[0]\n",
        "    \n",
        "acc = accuracy_score(test_target, test_labels)\n",
        "print(\"Accuracy using GMM = \", acc)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy using GMM =  0.5124760076775432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfBaMU1gIo7W",
        "outputId": "f22f638e-1f4d-4cc0-a096-0037ceb0552f"
      },
      "source": [
        "f1_Score_metric = f1_score(test_target,test_labels,average='weighted')\n",
        "print(\"F1 score \",f1_Score_metric)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score  0.347287040735797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGDCrLEH_8-k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM77mOThEqk1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNWgC5d9Eqnk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "805HXm6aEqqS"
      },
      "source": [
        "f1_Score_metric = f1_score(test['Target'],test_pred,average='weighted')\n",
        "print(\"F1 score \",f1_Score_metric)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
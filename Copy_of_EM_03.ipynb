{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EM_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjEMhUKiTtAC5sXMa7lNiO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssvadla/Research_01/blob/main/Copy_of_EM_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ylzT4-EaEff"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "MJaX0B4lWLL7",
        "outputId": "d4d202a6-4c34-4299-b442-0d6ce865725b"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train1 = pd.read_csv('/content/drive/My Drive/Research/train_data1.csv')\n",
        "train2 = pd.read_csv('/content/drive/My Drive/Research/train_data2.csv')\n",
        "train3 = pd.read_csv('/content/drive/My Drive/Research/train_data3.csv')\n",
        "train4 = pd.read_csv('/content/drive/My Drive/Research/train_data4.csv')\n",
        "train5 = pd.read_csv('/content/drive/My Drive/Research/train_data5.csv')\n",
        "train6 = pd.read_csv('/content/drive/My Drive/Research/train_data6.csv')\n",
        "train7 = pd.read_csv('/content/drive/My Drive/Research/train_data7.csv')\n",
        "train8 = pd.read_csv('/content/drive/My Drive/Research/train_data8.csv')\n",
        "train9 = pd.read_csv('/content/drive/My Drive/Research/train_data9.csv')\n",
        "train10 = pd.read_csv('/content/drive/My Drive/Research/train_data10.csv')\n",
        "train_highKappa = pd.read_csv('/content/drive/My Drive/Research/train_data_highkappa.csv')\n",
        "train1.head()\n",
        "\n",
        "train = train1\n",
        "train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "for i in train_list:\n",
        "  #print(i)\n",
        "  train = train.append(i)\n",
        "\n",
        "train.sort_values(\"Sentence\", inplace = True)\n",
        "print(len(train))\n",
        "new_train = train.drop_duplicates(subset =\"Sentence\")\n",
        "train = new_train\n",
        "train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "train['Target'].unique()\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl= WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "  text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+',text)\n",
        "  text = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "  return text\n",
        "\n",
        "len(train['Sentence'])\n",
        "text = clean_text(train['Sentence'])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "X_tfidf = tfidf_vect.fit_transform(train['Sentence'])\n",
        "print(X_tfidf.shape)\n",
        "\n",
        "test = pd.read_csv(r'/content/drive/My Drive/Research/test_data.csv')\n",
        "\n",
        "test['Target']=test['Target'].replace(['Others'],'Invalid')\n",
        "test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "test['Sentence'] = test['Sentence'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "t_p = tfidf_vect.transform(test['Sentence'])\n",
        "\n",
        "\n",
        "\n",
        "unlabel = pd.read_csv(r'/content/drive/My Drive/Research/Unlabeled_data.csv')\n",
        "#unlabel.head()\n",
        "\n",
        "del unlabel['Complete']\n",
        "del unlabel['Unnamed: 0']\n",
        "\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "unlabel['text'] = unlabel['text'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: TextBlob(x).words)\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "\n",
        "unlabel_1 = unlabel.loc[:500]\n",
        "\n",
        "\n",
        "def index_reset(unlabel_2):\n",
        "  unlabel_2.reset_index(inplace=True)\n",
        "  del unlabel_2['index']\n",
        "  #print(unlabel_2.head())\n",
        "  return unlabel_2\n",
        "\n",
        "unlabel_1 = index_reset(unlabel_1)\n",
        "unlabel_1_copy = unlabel_1\n",
        "\n",
        "\n",
        "\n",
        "x_un1 = tfidf_vect.transform(unlabel_1['text'])\n",
        "\n",
        "unlabel_1['Target']=-1\n",
        "\n",
        "train = train.rename(columns={'Sentence':'text'})\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "train['Target']= LabelEncoder().fit_transform(train['Target'])\n",
        "\n",
        "train_and_unlabel =  pd.concat([train,unlabel_1])\n",
        "print(len(train))\n",
        "print(len(unlabel_1))\n",
        "print(len(train_and_unlabel))\n",
        "\n",
        "train_and_unlabel.reset_index(inplace=True)\n",
        "del train_and_unlabel['index']\n",
        "del train_and_unlabel['Unnamed: 0']\n",
        "\n",
        "\n",
        "train_and_unlabel_words = clean_text(train_and_unlabel['text'])\n",
        "train_and_unlabel_words_wo_duplicates = list(set(train_and_unlabel_words))\n",
        "train_words = clean_text(train['text'])\n",
        "train_words_wo_duplicates = list(set(train_words))\n",
        "unlabel_words = clean_text(unlabel_1['text'])\n",
        "unlabel_words_wo_duplicates = list(set(unlabel_words))\n",
        "\n",
        "\n",
        "PF = []\n",
        "NF = []\n",
        "Threshold_feature = 1\n",
        "\n",
        "def feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words):\n",
        "  for term in train_and_unlabel_words_wo_duplicates:\n",
        "    print(term)\n",
        "    freq_train = train_words.count(term) / len(train_words)\n",
        "    print(freq_train)\n",
        "    freq_unlabel = unlabel_words.count(term) / len(unlabel_words)\n",
        "    print(freq_unlabel)\n",
        "    if freq_unlabel == 0:\n",
        "      PF.append(term)\n",
        "    elif (freq_train // freq_unlabel) > Threshold_feature:\n",
        "      PF.append(term)\n",
        "    else:\n",
        "      NF.append(term)\n",
        "\n",
        "\n",
        "feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def RN_selection(RN, unlabel_1):\n",
        "  iteration_RN = 0\n",
        "  RN_to_be_removed = []\n",
        "  pos_to_be_removed = []\n",
        "  freq_each_word_list = []\n",
        "  count = 0\n",
        "\n",
        "  for doc in unlabel_1['text']:\n",
        "    print(\"iteration_RN\",iteration_RN)\n",
        "    print(doc)\n",
        "    doc_words = clean_text(doc)\n",
        "    doc_words_wo_duplicates = list(set(doc_words))\n",
        "    for each_doc_word in doc_words_wo_duplicates:\n",
        "      print(each_doc_word)\n",
        "      freq_each_doc_word = doc_words.count(each_doc_word)\n",
        "      freq_each_word_list.append(freq_each_doc_word)\n",
        "      print(freq_each_doc_word)\n",
        "      print(each_doc_word in PF)\n",
        "      if (freq_each_doc_word > 0) and (each_doc_word in PF):\n",
        "        print(\"Yes writing ........\")\n",
        "        pos_to_be_removed.append(count)\n",
        "        RN_to_be_removed.append(doc)\n",
        "        print(\"breaking:::::::::::\")\n",
        "        break\n",
        "    count = count + 1\n",
        "    iteration_RN = iteration_RN + 1\n",
        "  \n",
        "  RN.drop(pos_to_be_removed,axis=0,inplace=True)\n",
        "  #Q =  unlabel_1.loc[pos_to_be_removed,:]\n",
        "  #print(len(Q))\n",
        "  return RN,pos_to_be_removed\n",
        "\n",
        "unlabel_2 = unlabel_1[0:10] #comment it \n",
        "RN = unlabel_2 #comment\n",
        "RN,pos_to_be_removed = RN_selection(RN,unlabel_2) # comment\n",
        "\n",
        "Q =  unlabel_1.loc[pos_to_be_removed,:]\n",
        "\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "def classifier_select(train, Q, RN):\n",
        "\n",
        "  loop_variable =0\n",
        "  #for i in range(0,3):\n",
        "  while(1):  # comment it change the range \n",
        "    print(loop_variable)\n",
        "    p_and_RN = pd.concat([train,RN])\n",
        "    print(\"#######target unique\",np.unique(p_and_RN['Target']))\n",
        "    p_and_RN.reset_index(inplace=True,drop=True)\n",
        "    p_and_RN_vect = tfidf_vect.transform(p_and_RN['text'])\n",
        "    p_and_RN_vect_df=pd.DataFrame(p_and_RN_vect.toarray())\n",
        "\n",
        "    Q_vect = tfidf_vect.transform(Q['text'])\n",
        "    Q_vect_df=pd.DataFrame(Q_vect.toarray())\n",
        "\n",
        "    lgb_classifier = lgb.LGBMClassifier()\n",
        "    lgb_classifier.fit(p_and_RN_vect_df, p_and_RN['Target'])\n",
        "    np.unique(p_and_RN['Target'])\n",
        "\n",
        "    #checking the classifier if it gives best results\n",
        "    train_vect = tfidf_vect.transform(train['text'])\n",
        "    train_vect_df=pd.DataFrame(train_vect.toarray())\n",
        "    train_pred = lgb_classifier.predict(train_vect_df)\n",
        "    print(\"#############train pred\", np.unique(train_pred))\n",
        "    classified_negative = (train_pred.tolist()).count(-1)\n",
        "    percentage = (classified_negative / len(train_pred)) * 100\n",
        "    model_list.append(lgb_classifier)\n",
        "    if percentage < percent_thresh:\n",
        "      #pass\n",
        "      thresh_model_list.append(lgb_classifier)\n",
        "      \n",
        "\n",
        "    Q_pred = lgb_classifier.predict(Q_vect_df)\n",
        "    np.unique(Q_pred)\n",
        "\n",
        "    \n",
        "\n",
        "    count_q = 0\n",
        "    total_q = 0 \n",
        "    out_pos_q = []\n",
        "    pos_q = []\n",
        "    for i in Q_pred:\n",
        "      #print(i)\n",
        "      if i == -1:\n",
        "        pos_q.append(count_q)\n",
        "        total_q = total_q + 1\n",
        "      else:\n",
        "        out_pos_q.append(count_q)\n",
        "\n",
        "      count_q = count_q + 1\n",
        "\n",
        "    Q.reset_index(inplace=True,drop=True)\n",
        "    W = Q.loc[pos_q,:]\n",
        "\n",
        "\n",
        "    if W.empty :\n",
        "      print(\"W is empty, came out of loop\")\n",
        "      break\n",
        "    else:\n",
        "      Q_new = Q.loc[out_pos_q,:]\n",
        "      Q = Q_new\n",
        "      RN = pd.concat([RN,W])\n",
        "      RN.reset_index(inplace=True,drop=True)\n",
        "      loop_variable = loop_variable + 1\n",
        "    print(\"completed iteration\")\n",
        "    \n",
        "\n",
        "  return lgb_classifier, model_list , thresh_model_list\n",
        "\n",
        "percent_thresh = 5\n",
        "model_list = []\n",
        "thresh_model_list = []\n",
        "lgb_classifier, op_model_list, op_thresh_model_list = classifier_select(train, Q, RN)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "for model in op_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  print(classification_report_test)\n",
        "\n",
        "for model in op_thresh_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  print(classification_report_test)\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>659</td>\n",
              "      <td>Appellant had stated to the officers that she ...</td>\n",
              "      <td>Invalid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3456</td>\n",
              "      <td>We shall discuss the facts more fully in conne...</td>\n",
              "      <td>Others</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2043</td>\n",
              "      <td>â€œPerjury is a false statement, either writte...</td>\n",
              "      <td>Invalid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3344</td>\n",
              "      <td>The offense is felony theft by false pretext; ...</td>\n",
              "      <td>Issue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3231</td>\n",
              "      <td>Numerous contentions urging the commission of ...</td>\n",
              "      <td>Issue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                           Sentence   Target\n",
              "0         659  Appellant had stated to the officers that she ...  Invalid\n",
              "1        3456  We shall discuss the facts more fully in conne...   Others\n",
              "2        2043  â€œPerjury is a false statement, either writte...  Invalid\n",
              "3        3344  The offense is felony theft by false pretext; ...    Issue\n",
              "4        3231  Numerous contentions urging the commission of ...    Issue"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8BxnWUufERI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_VQp63uhKft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-53aEmQyj0gW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlmRt4vB3Eru",
        "outputId": "79c9f216-83b8-4585-e0dc-3a9c9be92fb1"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "for model in op_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  print(classification_report_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3922    0.2597    0.3125        77\n",
            "           1     0.6957    0.6154    0.6531        26\n",
            "           2     0.6743    0.8839    0.7650       267\n",
            "           3     0.5263    0.3614    0.4286        83\n",
            "           4     0.9167    0.3235    0.4783        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6315       521\n",
            "   macro avg     0.6294    0.4858    0.5256       521\n",
            "weighted avg     0.6192    0.6315    0.6040       521\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTTcNIh3aRj",
        "outputId": "8be6310a-e366-491c-e8a3-37ec49f3819e"
      },
      "source": [
        "for model in op_thresh_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  print(classification_report_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3922    0.2597    0.3125        77\n",
            "           1     0.6957    0.6154    0.6531        26\n",
            "           2     0.6743    0.8839    0.7650       267\n",
            "           3     0.5263    0.3614    0.4286        83\n",
            "           4     0.9167    0.3235    0.4783        34\n",
            "           5     0.5714    0.4706    0.5161        34\n",
            "\n",
            "    accuracy                         0.6315       521\n",
            "   macro avg     0.6294    0.4858    0.5256       521\n",
            "weighted avg     0.6192    0.6315    0.6040       521\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz2SNE5F4PPg"
      },
      "source": [
        "np.unique(RN['Target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NafiIgbrYNuX"
      },
      "source": [
        "test_pred =  lgb_classifier.predict(t_p.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq38owvZYQop"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "print(classification_report_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwHlCuJbYgF0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}